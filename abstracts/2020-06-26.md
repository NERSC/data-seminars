# Congestion and Distributed Training of Deep Neural Networks
## Jacob Balma (HPE)

## Abstract 
Congestion is one of the biggest problems facing large-scale multi-user HPC systems today. It results in fewer jobs being run per dollar spent over the life of the system, affecting not only the systemâ€™s total throughput, but also user experience. Loaded systems experience congestion in the form of run-to-run variability and contention for shared resources like filesystems or routes between compute endpoints. Emerging workloads in data-science and specifically deep learning can exacerbate contention for shared resources by requiring high-bandwidth between I/O subsystems and compute nodes simultaneously. Current network benchmarks are not capable of creating the conditions necessary to proxy real-world network utilization seen on congested systems, nor do they offer mechanisms for quantifying the impact it has on user applications which run under such conditions. A recently proposed open-source set of network benchmarks called the Global Performance and Congestion Network Tests (GPCNeT) are aimed at providing an industry standard for performance characterization of well-utilized HPC networks. We expand the GPCNeT benchmark suite to support our investigation of the communication patterns intrinsic to Synchronous Distributed SGD, commonly used to train Deep Neural Networks (DNNs) on distributed systems.

We show that Synchronous Distributed SGD can cause Congestion when run alongside the Congestion-Sensitive algorithms used by GPCNeT to proxy prominent features associated with many common HPC workloads. Finally, we compare the degree to which hyper-parameter tuning algorithms when applied to Synchronous Distributed SGD affect the congestion impact factor relative to various sensitive workloads.

## Bio
Jacob Balma works as an HPC & AI Research Scientist at HPE. His background is in statistical physics, scientific simulation and benchmarking. Current interests include drug discovery, open-therapeutics and materials science.
